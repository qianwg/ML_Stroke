{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f67daec-b28d-4635-8567-bc4d43f67ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOAD PACKAGES ###\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.stats import kruskal,chisquare\n",
    "import altair as alt\n",
    "import ugtm\n",
    "import textwrap\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster import hierarchy \n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import os\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from tqdm.auto import tqdm\n",
    "from itertools import chain\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72d02362-ac6d-4b61-9f67-c6ed158999ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# 忽略所有 SettingWithCopyWarning\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6fb1d985-435f-4efc-8fb9-7dd24cfbc4a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:34:05.889725Z",
     "start_time": "2020-11-04T12:34:05.874587Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_fields(fields, data, data_field):\n",
    "    f = data_field[data_field[\"field.showcase\"].isin(fields) & data_field[\"field.tab\"].str.contains(\"f\\\\.\\\\d+\\\\.0\\\\.\\\\d\")].copy()\n",
    "    f[\"field\"] = pd.Categorical(f[\"field.showcase\"], categories=fields, ordered=True)\n",
    "    f = f.sort_values(\"field\").reset_index().drop(\"field\", axis=1)\n",
    "    return f\n",
    "\n",
    "def get_fields_all(fields, data, data_field):\n",
    "    f = data_field[data_field[\"field.showcase\"].isin(fields)].copy()\n",
    "    f[\"field\"] = pd.Categorical(f[\"field.showcase\"], categories=fields, ordered=True)\n",
    "    f = f.sort_values(\"field\").reset_index().drop(\"field\", axis=1)\n",
    "    return f\n",
    "\n",
    "def get_data_fields(fields, data, data_field):\n",
    "    f = get_fields(fields, data, data_field)\n",
    "    return data[[\"eid\"]+f[\"col.name\"].to_list()].copy()\n",
    "\n",
    "def get_data_fields_all(fields, data, data_field):\n",
    "    f = get_fields_all(fields, data, data_field)\n",
    "    return data[[\"eid\"]+f[\"col.name\"].to_list()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96ade741-cf88-444c-89f0-f8259172562b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def map_column_to_meaning(df, column_name, data_path, file):\n",
    "    # 读取数据文件\n",
    "    coding1001 = pd.read_csv(f\"{data_path}/{file}\", sep=\"\\t\")\n",
    "    \n",
    "    # 将 coding 列转换为字符串类型\n",
    "    coding1001['coding'] = coding1001['coding'].astype('str')\n",
    "    \n",
    "    # 将列重命名为指定的 column_name\n",
    "    coding1001.rename(columns={\"coding\": column_name}, inplace=True)\n",
    "    \n",
    "    # 将 column_name 列转换为字符串类型\n",
    "    df[column_name] = df[column_name].astype('str')\n",
    "    \n",
    "    # 创建 code 到 meaning 的映射字典\n",
    "    code_to_meaning = dict(zip(coding1001[column_name], coding1001['meaning']))\n",
    "    \n",
    "    # 使用映射字典替换 column_name 列的值，并将其转换为分类类型\n",
    "    df[column_name] = df[column_name].map(code_to_meaning).astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7f3e3a02-0a03-423a-97ae-d55daaf4a1e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def datetime_from_dec_year(dec_year):\n",
    "    start = dec_year\n",
    "    year = int(start)\n",
    "    rem = start - year\n",
    "\n",
    "    base = datetime(year, 1, 1)\n",
    "    result = base + timedelta(seconds=(base.replace(year=base.year + 1) - base).total_seconds() * rem)\n",
    "    #result.strftime(\"%Y-%m-%d\")\n",
    "    return result.date()\n",
    "\n",
    "def extract_map_self_reported(data, data_field, code_map):\n",
    "    pbar = tqdm(total=16)\n",
    "    ### codes\n",
    "    fields = [\"20002\"]; pbar.update(1)\n",
    "    raw = get_data_fields_all(fields, data, data_field); pbar.update(1)\n",
    "    col = \"noncancer_illness_code_selfreported_f20002\"; pbar.update(1)\n",
    "    temp = pd.wide_to_long(raw, stubnames=[col], i=\"eid\", j=\"instance_index\", sep=\"_\", suffix=\"\\w+\").reset_index(); pbar.update(1)\n",
    "    codes = temp.rename(columns={col:\"code\"})\\\n",
    "        .assign(code=lambda x: x.code.astype(str))\\\n",
    "        .replace(\"None\", np.nan) \\\n",
    "        .replace(\"nan\", np.nan) \\\n",
    "        .dropna(subset=[\"code\"], axis=0)\\\n",
    "        .assign(code=lambda x: x.code.astype(int)) \\\n",
    "        .merge(code_map, how=\"left\",on=\"code\") \\\n",
    "        .sort_values([\"eid\", \"instance_index\"]) \\\n",
    "        .reset_index(drop=True); pbar.update(1)\n",
    "    \n",
    "    ### dates\n",
    "    fields = [\"20008\"]; pbar.update(1)\n",
    "    raw = get_data_fields_all(fields, data, data_field); pbar.update(1)\n",
    "    col=\"interpolated_year_when_noncancer_illness_first_diagnosed_f20008\"; pbar.update(1)\n",
    "    temp = pd.wide_to_long(raw, stubnames=[col], i=\"eid\", j=\"instance_index\", sep=\"_\", suffix=\"\\w+\").reset_index(); pbar.update(1)\n",
    "    dates = temp.rename(columns={col:\"date\"})\\\n",
    "        .dropna(subset=[\"date\"], axis=0)\\\n",
    "        .sort_values([\"eid\", \"instance_index\"]) \\\n",
    "        .reset_index(drop=True); pbar.update(1)\n",
    "\n",
    "    dates = dates[dates.date!=-1]; pbar.update(1)\n",
    "    dates = dates[dates.date!=-3]; pbar.update(1)\n",
    "    dates.date = dates.date.apply(datetime_from_dec_year); pbar.update(1)\n",
    "    \n",
    "    test = codes.merge(dates, how=\"left\", on=[\"eid\", \"instance_index\"]).assign(origin=\"self_reported\").copy(); pbar.update(1)\n",
    "    \n",
    "    test[\"instance_index\"] = test[\"instance_index\"].astype(\"string\"); pbar.update(1)\n",
    "    test[['instance','n']] = test.instance_index.str.split(\"_\",expand=True); pbar.update(1)\n",
    "    pbar.close()\n",
    "    \n",
    "    return test[[\"eid\", \"origin\", 'instance','n', \"code\", \"meaning\", \"date\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "000dfe1b-877e-41c5-a624-cee75d2c3b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_geno_path = \"D:/UKBiobank/Geno\"\n",
    "data_base_path = \"D:/UKBiobank/brain/2_datasets_pre\"\n",
    "save_data_path = \"D:/UKBiobank/ML_Stroke/Data\"\n",
    "data_sup1_path = \"D:/UKBiobank/Sup1\"\n",
    "data_sup4_path = \"D:/UKBiobank/Sup4\"\n",
    "data_sup5_path = \"D:/UKBiobank/Sup5\"\n",
    "data_exam_path = \"D:/UKBiobank/Exam\"\n",
    "data_path = \"D:/UKBiobank/Green\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b2d0cea-b846-4d88-acd0-d885c2554c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "ukbiobank_variable_definitions = pd.read_csv(\"D:/UKBiobank/AF_PHENOTYPE_GTM/data/ukbiobank/ukbiobank_variable_definitions.csv\")\n",
    "phecode_df = pd.read_csv(\"D:/UKBiobank/AF_PHENOTYPE_GTM/data/ukbiobank/phecode_icd10_mappings.csv\", encoding=\"latin_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e2ebfd9e-5d9a-4473-b450-44888e7cda2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_stroke = pd.read_feather(f\"{save_data_path}/temp_stroke_diagnosis_image.feather\")\n",
    "var_df = pd.read_feather(f\"{save_data_path}/temp_cmr.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4ef852e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = var_df.iloc[:,1:]\n",
    "var_df_notna = var_df[selected_columns.notna().all(axis=1)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e8ba5e37-c7d4-44d7-9172-446bb3410531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(temp_stroke.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "15e0f0ce-073f-4718-9c97-2032ab1bc699",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vis_df = temp_stroke.iloc[:, [0,1,4,7,10,13,16,19,22,25,28,31,34,37,40]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5b879e96-6d68-4730-b920-f857ce4d5bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['eid', 'ALL Stroke_event', 'Self report - Stroke_event',\n",
       "       'Self report - Subarachnoid haemorrhage_event',\n",
       "       'Self report - Brain haemorrhage_event',\n",
       "       'Self report - Ischaemic stroke_event',\n",
       "       'ICD 9 - Subarachnoid haemorrhage_event',\n",
       "       'ICD 9 - Intracerebral haemorrhage_event',\n",
       "       'ICD 9 - Occlusion of cerebral arteries_event',\n",
       "       'ICD 9 - Acute, but ill-defined, cerebrovascular disease_event',\n",
       "       'ICD 10 - Subarachnoid haemorrhage_event',\n",
       "       'ICD 10 - Intracerebral haemorrhage_event',\n",
       "       'ICD 10 - Other nontraumatic intracranial haemorrhage_event',\n",
       "       'ICD 10 - Cerebral infarction_event',\n",
       "       'ICD 10 - Stroke, not specified as haemorrhage or infarction_event'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_vis_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6fb23fd8-9214-485b-8622-daa5d04c916c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = [model_vis_df.columns[0]] + [col.replace('_event', '') for col in model_vis_df.columns[1:]]\n",
    "model_vis_df.columns = new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "424cecc0-377f-4e1d-8425-a48a15084552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get AF definition for each participants\n",
    "stroke_definition_vis = model_vis_df\n",
    "\n",
    "# get the variables for modeling\n",
    "modelling_df = var_df_notna[var_df_notna['eid'].isin(model_vis_df['eid'])].reset_index(drop=True)\n",
    "modelling_df_NOID = modelling_df.drop(\"eid\", axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e94560b",
   "metadata": {},
   "source": [
    "# The number of participants with CMR examination was 37898; the number of participants with stroke was 13378; the overlap number was 642; This is nothing\n",
    "# The final purpose of CMR examization was to identify clusters of participants.\n",
    "# The target population was xxx? How to define the population? The stroke patients? or the general population or the healthy population?\n",
    "# The UK Biobank was a study about the general population. So the clustering must be the general population with furture risk of some kind of disease?\n",
    "# Clustering the general population excluding participants with disease at baseline? \n",
    "\n",
    "\n",
    "# The goal was clustering CMR in patients free of stroke before imaging visiting and find the cluster with higher risk of stroke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "df1d7299-e215-447b-9694-f336bbf0829d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_indices(list1, list2):\n",
    "    return [i for i, x in enumerate(list2) if x in list1]\n",
    "\n",
    "# Extract the column positions for data that need to be log transformed\n",
    "log_vars = list(ukbiobank_variable_definitions[\"UDI\"].loc[ukbiobank_variable_definitions[\"log_transform\"]==1].values)\n",
    "\n",
    "all_model_vars = modelling_df_NOID.columns\n",
    "\n",
    "log_pos = find_indices(log_vars, all_model_vars)\n",
    "\n",
    "# Impute missing values within the dataset\n",
    "imp = IterativeImputer(max_iter=10, random_state=0)\n",
    "\n",
    "# Fit the imputer on the dataset\n",
    "imp.fit(modelling_df_NOID)\n",
    "\n",
    "# Transform the dataset (impute the missing values)\n",
    "X_transformed = imp.transform(modelling_df_NOID)\n",
    "\n",
    "for pos in log_pos:\n",
    "    X_transformed[:,pos] = np.log(X_transformed[:,pos]+1-np.min(X_transformed[:,pos]))\n",
    "\n",
    "data_df_impute = pd.DataFrame(X_transformed)\n",
    "data_df_impute.columns = modelling_df_NOID.columns\n",
    "\n",
    "# Scale and tranform the data\n",
    "scaler = StandardScaler().fit(data_df_impute)\n",
    "Xvis = scaler.transform(data_df_impute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2b8af326-cc54-47ab-8f37-bacb0842c8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "from ugtm import runGTM\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "67b88d28-edd6-4812-a076-2551bb3a56cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for UGTM pipeline: {'ugtm__k': 16, 'ugtm__m': 4, 'ugtm__regul': 0.001, 'ugtm__s': 0.2}\n",
      "Best NLL for UGTM pipeline: -5423331.65468949\n"
     ]
    }
   ],
   "source": [
    "# 自定义负对数似然评分函数\n",
    "def negative_log_likelihood_scorer(estimator, X):\n",
    "    log_likelihood = estimator.score(X)\n",
    "    return -log_likelihood * len(X)\n",
    "\n",
    "class UGTMTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, k=15,m=14,regul=1,s=0.1, random_state=None):\n",
    "        self.k = k\n",
    "        self.regul = regul\n",
    "        self.random_state = random_state\n",
    "        self.m = m\n",
    "        self.s = s\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.ugtm_ = runGTM(X, k=self.k, random_state=self.random_state, m=self.m,regul=self.regul, s = self.s)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return MinMaxScaler().fit_transform(self.ugtm_.matY.T).T\n",
    "\n",
    "\n",
    "# 创建管道\n",
    "pipe_ugtm = Pipeline([('ugtm', UGTMTransformer(random_state=42)), ('gmm', GaussianMixture(random_state=42))])\n",
    "\n",
    "# 参数设置\n",
    "param_grid_ugtm = {\n",
    "    'ugtm__k': [16], \n",
    "    'ugtm__m': [4],  \n",
    "    'ugtm__s': [0.1, 0.2, 0.3],\n",
    "    'ugtm__regul':[0.0001, 0.001, 0.01,0.1],\n",
    "    #'gmm__n_components': [2, 3, 4],  # 搜索 2, 3 和 4 组件的 GMM\n",
    "    #'gmm__covariance_type': ['full', 'tied', 'diag', 'spherical']  # GMM 的协方差类型\n",
    "}\n",
    "\n",
    "# 执行 GridSearchCV\n",
    "#grid_search_pca = GridSearchCV(pipe_pca, param_grid_pca, scoring=negative_log_likelihood_scorer, cv=10)\n",
    "#grid_search_tsne = GridSearchCV(pipe_tsne, param_grid_tsne, scoring=negative_log_likelihood_scorer, cv=10)\n",
    "grid_search_ugtm = GridSearchCV(pipe_ugtm, param_grid_ugtm, scoring=negative_log_likelihood_scorer, cv=10)\n",
    "\n",
    "# 拟合模型并搜索最佳参数\n",
    "#grid_search_pca.fit(X)\n",
    "#grid_search_tsne.fit(X)\n",
    "grid_search_ugtm.fit(Xvis)\n",
    "\n",
    "# 输出最佳参数和对应的 NLL 得分\n",
    "#print(\"Best parameters for PCA pipeline:\", grid_search_pca.best_params_)\n",
    "#print(\"Best NLL for PCA pipeline:\", grid_search_pca.best_score_)\n",
    " \n",
    "#print(\"Best parameters for t-SNE pipeline:\", grid_search_tsne.best_params_)\n",
    "#print(\"Best NLL for t-SNE pipeline:\", grid_search_tsne.best_score_)\n",
    "\n",
    "print(\"Best parameters for UGTM pipeline:\", grid_search_ugtm.best_params_)\n",
    "print(\"Best NLL for UGTM pipeline:\", grid_search_ugtm.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ce45f1-f384-49a0-8fb9-3a83095fa33d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GTM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
